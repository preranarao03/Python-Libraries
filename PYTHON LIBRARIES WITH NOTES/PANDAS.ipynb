{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae43d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# PANDAS package provides fast, flexible, and expressive data structures.\n",
    "# Designed to work with \"relational\" or \"labeled\" data\n",
    "# Has functions for analyzing, cleaning, exploring and manipulating data\n",
    "\n",
    "#APPLICATIONS:\n",
    "# Easy handling of missing data\n",
    "# Size mutability: columns can be inserted and deleted from Dataframe\n",
    "# Automatic and explicit data alignment\n",
    "# Intelligent label-based slicing, fancy indexing \n",
    "# Intutive merging and joining data sets\n",
    "# Flexible reshaping and pivoting data sets\n",
    "\n",
    "#PANDAS STRUCTURES: \n",
    "# PANDAS data structures act like containers for lower dimensional data\n",
    "# DataFrames is a container for Series\n",
    "# Series is a container for scalars\n",
    "# We would like to insert and remove objects from these containers in a dictionary-like fashion\n",
    "\n",
    "#PANDAS Series:\n",
    "# PANDAS Series is a 1-D labeled array capable of holding data of any type\n",
    "# PANDAS Series is nothing but a column in an excel sheet\n",
    "\n",
    "#PANDAS DataFrames:\n",
    "# PANDAS DataFrame is  2-D size-mutable data structure with labeled axes (rows and columns)\n",
    "# DataFrame consists of 3 principal components - the data,rows and columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147162ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name  Age  Salary\n",
      "0   peru   23    4500\n",
      "1   john   45    5678\n",
      "2  pavan   67    4354\n"
     ]
    }
   ],
   "source": [
    "#CREATION OF DATAFRAMES IN PANDAS\n",
    "\n",
    "data={\"Name\":[\"peru\",\"john\",\"pavan\"], \n",
    "      \"Age\":[23,45,67],\n",
    "     \"Salary\":[4500,5678,4354]} #data in dictionary form\n",
    "\n",
    "df=pd.DataFrame(data) #pd.DataFrame() used to convert data to DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb5f359",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# used to read csv file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ' '"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\" \") # used to read csv file\n",
    "data=pd.read_excel(\" \") #used to read excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e88e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date    Category       Sub-Category   Amount Payment Mode\n",
      "0  2023-01-01     Grocery             Grocery      30         Cash\n",
      "1  2023-01-02        Food          Restaurant     890          UPI\n",
      "2  2023-01-04         123              Zomato     257          NaN\n",
      "3  2023-01-06  Essentials               Diary     120          UPI\n",
      "4  2023-01-06  Essentials             Perfume    1500         Cash\n",
      "5  2023-01-09     Grocery  Fruits and Veggies     456         Cash\n",
      "6  2023-01-10       Bills          House Rent   16000          UPI\n",
      "7  2023-01-10     Grocery      Tomato KetchUp      70          UPI\n",
      "8  2023-01-12        Food                Chai      15          UPI\n",
      "9  2023-01-15  Essentials      Salt and Sugar      50          NaN\n",
      "10 2023-01-17     Grocery           Chocolate     100          UPI\n",
      "11 2023-01-17        Food          Restaurant     780         Card\n",
      "12 2023-01-18  Essentials            Food Oil     120          NaN\n",
      "13 2023-01-18        Food              Zomato     230          UPI\n",
      "14 2023-01-19     Grocery                Milk      26          UPI\n",
      "15 2023-01-20  Essentials             Shampoo     780          UPI\n",
      "16 2023-01-20  Essentials           Lunch Box     890         Cash\n",
      "17 2023-01-21     Clothes               Dress    1000          NaN\n",
      "18 2023-01-22     Clothes               Dress    1890          UPI\n",
      "19 2023-01-23     Grocery      Bread and Milk      56         Cash\n",
      "20 2023-01-24        Food  Fruits and Veggies     530         Cash\n",
      "21 2023-01-26        Food                Chai      10          UPI\n",
      "22 2023-01-26     Grocery               Maggi     140          UPI\n",
      "23 2023-01-27        Food              Zomato     300          UPI\n",
      "24 2023-01-27        Food                Chai      10          UPI\n",
      "25 2023-01-28  Essentials           Bedsheets    1025         Cash\n",
      "26 2023-01-29       Bills             Mobile     1650          UPI\n",
      "27 2023-01-29     Grocery                Daal     150         Cash\n",
      "28 2023-01-30       Bills            Cylinder    1074          UPI\n",
      "        Date    Category       Sub-Category   Amount Payment Mode\n",
      "0 2023-01-01     Grocery             Grocery      30         Cash\n",
      "1 2023-01-02        Food          Restaurant     890          UPI\n",
      "2 2023-01-04         123              Zomato     257          NaN\n",
      "3 2023-01-06  Essentials               Diary     120          UPI\n",
      "4 2023-01-06  Essentials             Perfume    1500         Cash\n",
      "5 2023-01-09     Grocery  Fruits and Veggies     456         Cash\n",
      "6 2023-01-10       Bills          House Rent   16000          UPI\n",
      "7 2023-01-10     Grocery      Tomato KetchUp      70          UPI\n",
      "8 2023-01-12        Food                Chai      15          UPI\n",
      "9 2023-01-15  Essentials      Salt and Sugar      50          NaN\n",
      "         Date    Category       Sub-Category   Amount Payment Mode\n",
      "19 2023-01-23     Grocery      Bread and Milk      56         Cash\n",
      "20 2023-01-24        Food  Fruits and Veggies     530         Cash\n",
      "21 2023-01-26        Food                Chai      10          UPI\n",
      "22 2023-01-26     Grocery               Maggi     140          UPI\n",
      "23 2023-01-27        Food              Zomato     300          UPI\n",
      "24 2023-01-27        Food                Chai      10          UPI\n",
      "25 2023-01-28  Essentials           Bedsheets    1025         Cash\n",
      "26 2023-01-29       Bills             Mobile     1650          UPI\n",
      "27 2023-01-29     Grocery                Daal     150         Cash\n",
      "28 2023-01-30       Bills            Cylinder    1074          UPI\n"
     ]
    }
   ],
   "source": [
    "#EXPLORING DATA IN PANDAS\n",
    "data = pd.read_excel(\"expense3.xlsx\")\n",
    "print(data)\n",
    "print(data.head(10)) # .head() allows us to print specific first number of rows \n",
    "                     # ex - we want to print the first 10 rows \n",
    "    \n",
    "print(data.tail(10)) # .tail() allows us to print the last specific number of rows of data\n",
    "                     # ex - we want to print the last 10 rows\n",
    "# if we dont pass a specific number in .head() and .tail() we will by default get 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c74380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29 entries, 0 to 28\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Date           29 non-null     datetime64[ns]\n",
      " 1   Category       29 non-null     object        \n",
      " 2   Sub-Category   29 non-null     object        \n",
      " 3   Amount         29 non-null     int64         \n",
      " 4   Payment Mode   25 non-null     object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 1.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info()) # to get info (no. of non-null values, data-type etc.) on each of the columns we use .info()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44975ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Date        Amount\n",
      "count                             29     29.000000\n",
      "mean   2023-01-17 19:02:04.137931008   1039.620690\n",
      "min              2023-01-01 00:00:00     10.000000\n",
      "25%              2023-01-10 00:00:00     70.000000\n",
      "50%              2023-01-19 00:00:00    257.000000\n",
      "75%              2023-01-26 00:00:00    890.000000\n",
      "max              2023-01-30 00:00:00  16000.000000\n",
      "std                              NaN   2927.684353\n"
     ]
    }
   ],
   "source": [
    "print(data.describe()) #for the numerical data columns .describe() will provide info like count, mean, min etc.\n",
    "# std - standard deviation\n",
    "# 25%, 50%, 75% - these are percentile values\n",
    "# ex - in example in 25% amount=70, this means 25% have amount less than 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4578233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Date  Category  Sub-Category   Amount  Payment Mode\n",
      "0   False     False          False   False         False\n",
      "1   False     False          False   False         False\n",
      "2   False     False          False   False          True\n",
      "3   False     False          False   False         False\n",
      "4   False     False          False   False         False\n",
      "5   False     False          False   False         False\n",
      "6   False     False          False   False         False\n",
      "7   False     False          False   False         False\n",
      "8   False     False          False   False         False\n",
      "9   False     False          False   False          True\n",
      "10  False     False          False   False         False\n",
      "11  False     False          False   False         False\n",
      "12  False     False          False   False          True\n",
      "13  False     False          False   False         False\n",
      "14  False     False          False   False         False\n",
      "15  False     False          False   False         False\n",
      "16  False     False          False   False         False\n",
      "17  False     False          False   False          True\n",
      "18  False     False          False   False         False\n",
      "19  False     False          False   False         False\n",
      "20  False     False          False   False         False\n",
      "21  False     False          False   False         False\n",
      "22  False     False          False   False         False\n",
      "23  False     False          False   False         False\n",
      "24  False     False          False   False         False\n",
      "25  False     False          False   False         False\n",
      "26  False     False          False   False         False\n",
      "27  False     False          False   False         False\n",
      "28  False     False          False   False         False\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull()) #if value is null - True will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5c89c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date             0\n",
      "Category         0\n",
      "Sub-Category     0\n",
      "Amount           0\n",
      "Payment Mode     4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum()) #this will tell us the total number of null values under each specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d579ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F      NaN\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M      NaN\n",
      "6  EMP02     rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "#DEALING WITH DUPLICATE VALUES IN PANDAS\n",
    "\n",
    "data = pd.read_csv(\"company1.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183d91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "5    False\n",
      "6     True\n",
      "dtype: bool\n",
      " \n",
      "1\n",
      " \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data.duplicated()) #.duplicated() used to identify the duplicated data\n",
    "                         #True means that data is duplicate\n",
    "print(\" \")\n",
    "print(data.duplicated().sum()) #tells the total number of duplicated values\n",
    "print(\" \")\n",
    "\n",
    "#to get the total number of duplicate values in a particular column, specify the column\n",
    "print(data['EEID'].duplicated().sum()) #there are 2 duplicate values in column EEID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "971d4e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F      NaN\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M      NaN\n",
      " \n",
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F      NaN\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "4  EMP05       NaN      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "#to drop the duplicates in a particular column: \n",
    "print(data.drop_duplicates('EEID'))#specify which column you want the duplicates to removed from\n",
    "print(\" \")\n",
    "print(data.drop_duplicates('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4278560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F      NaN\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M      NaN\n",
      "6  EMP02     rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "#WORKING WITH MISSING DATA IN PANDAS\n",
    "data = pd.read_csv(\"company1.csv\")\n",
    "print(data) #NaN values represent an empty value(Not a Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57ce516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID    Name gender   salary\n",
      "1  EMP02   rohit      M  25000.0\n",
      "3  EMP01  ayushi      F  20000.0\n",
      "6  EMP02   rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "# to drop the null values:\n",
    "print(data.dropna()) #.dropna() used to drop the null values in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da4c94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F       hi\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali     hi  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05        hi      M  25000.0\n",
      "5  EMP06     rohit      M       hi\n",
      "6  EMP02     rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "# we can replace NaN with some value using .replace()\n",
    "# .replace() is a numpy function, so we should import numpy first\n",
    "\n",
    "import numpy as np\n",
    "print(data.replace(np.nan,\"hi\")) #.replace(what we want to replace, what we want to replace it with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0899243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F   3000.0\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M   3000.0\n",
      "6  EMP02     rohit      M  25000.0\n",
      " \n",
      "18285.714285714286\n"
     ]
    }
   ],
   "source": [
    "#suppose we want to replace the Nan of only a particular column\n",
    "data = pd.read_csv(\"company1.csv\")\n",
    "data[\"salary\"]=data[\"salary\"].replace(np.nan,3000)\n",
    "print(data) #the NaN under salary column will be replaced by 3000\n",
    "\n",
    "#if we have to replace a numerical data, by new data it would be better\n",
    "# cont.- if we replace by its mean\n",
    "print(\" \")\n",
    "mean_salary=(data[\"salary\"].mean())\n",
    "print(mean_salary)\n",
    "data[\"salary\"]=data[\"salary\"].replace(np.nan,18285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1a57e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F   3000.0\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M   3000.0\n",
      "6  EMP02     rohit      M  25000.0\n",
      " \n",
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F   3000.0\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali      F  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05     rohit      M  25000.0\n",
      "5  EMP06     rohit      M   3000.0\n",
      "6  EMP02     rohit      M  25000.0\n",
      " \n",
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F   3000.0\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali      M  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05    ayushi      M  25000.0\n",
      "5  EMP06     rohit      M   3000.0\n",
      "6  EMP02     rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "# to fill the NaN values there are 2 ways\n",
    "# backward fill (bfill) & forward fill (ffill)\n",
    "print(data)\n",
    "print(\" \")\n",
    "\n",
    "#if we want the data after the NaN to be filled into the NaN slot use bfill\n",
    "print(data.fillna(method=\"bfill\"))\n",
    "print(\" \")\n",
    "\n",
    "#if we want the data before the NaN to be filled into the NaN slot use ffill\n",
    "print(data.fillna(method=\"ffill\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5564bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       EEID        Full Name                 Job Title  Department  \\\n",
      "0    E02387      Emily Davis                Sr. Manger          IT   \n",
      "1    E04105    Theodore Dinh       Technical Architect          IT   \n",
      "2    E02572     Luna Sanders                  Director     Finance   \n",
      "3    E02832  Penelope Jordan  Computer Systems Manager          IT   \n",
      "4    E01639        Austin Vo               Sr. Analyst     Finance   \n",
      "..      ...              ...                       ...         ...   \n",
      "995  E03094     Wesley Young               Sr. Analyst   Marketing   \n",
      "996  E01909     Lillian Khan                   Analyst     Finance   \n",
      "997  E04398      Oliver Yang                  Director   Marketing   \n",
      "998  E02521      Lily Nguyen               Sr. Analyst     Finance   \n",
      "999  E03545      Sofia Cheng            Vice President  Accounting   \n",
      "\n",
      "              Business Unit  Gender  Ethnicity  Age  Hire Date  Annual Salary  \\\n",
      "0    Research & Development  Female      Black   55 2016-04-08         141604   \n",
      "1             Manufacturing    Male      Asian   59 1997-11-29          99975   \n",
      "2       Speciality Products  Female  Caucasian   50 2006-10-26         163099   \n",
      "3             Manufacturing  Female  Caucasian   26 2019-09-27          84913   \n",
      "4             Manufacturing    Male      Asian   55 1995-11-20          95409   \n",
      "..                      ...     ...        ...  ...        ...            ...   \n",
      "995     Speciality Products    Male  Caucasian   33 2016-09-18          98427   \n",
      "996     Speciality Products  Female      Asian   44 2010-05-31          47387   \n",
      "997     Speciality Products    Male      Asian   31 2019-06-10         176710   \n",
      "998     Speciality Products  Female      Asian   33 2012-01-28          95960   \n",
      "999               Corporate  Female      Asian   63 2020-07-26         216195   \n",
      "\n",
      "     Bonus %        Country       City  Exit Date  \n",
      "0       0.15  United States    Seattle 2021-10-16  \n",
      "1       0.00          China  Chongqing        NaT  \n",
      "2       0.20  United States    Chicago        NaT  \n",
      "3       0.07  United States    Chicago        NaT  \n",
      "4       0.00  United States    Phoenix        NaT  \n",
      "..       ...            ...        ...        ...  \n",
      "995     0.00  United States   Columbus        NaT  \n",
      "996     0.00          China    Chengdu 2018-01-08  \n",
      "997     0.15  United States      Miami        NaT  \n",
      "998     0.00          China    Chengdu        NaT  \n",
      "999     0.31  United States      Miami        NaT  \n",
      "\n",
      "[1000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "#COLUMN TRANSFORMATION IN PANDAS\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"ESD.xlsx\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ead68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       EEID        Full Name                 Job Title  Department  \\\n",
      "0    E02387      Emily Davis                Sr. Manger          IT   \n",
      "1    E04105    Theodore Dinh       Technical Architect          IT   \n",
      "2    E02572     Luna Sanders                  Director     Finance   \n",
      "3    E02832  Penelope Jordan  Computer Systems Manager          IT   \n",
      "4    E01639        Austin Vo               Sr. Analyst     Finance   \n",
      "..      ...              ...                       ...         ...   \n",
      "995  E03094     Wesley Young               Sr. Analyst   Marketing   \n",
      "996  E01909     Lillian Khan                   Analyst     Finance   \n",
      "997  E04398      Oliver Yang                  Director   Marketing   \n",
      "998  E02521      Lily Nguyen               Sr. Analyst     Finance   \n",
      "999  E03545      Sofia Cheng            Vice President  Accounting   \n",
      "\n",
      "              Business Unit  Gender  Ethnicity  Age  Hire Date  Annual Salary  \\\n",
      "0    Research & Development  Female      Black   55 2016-04-08         141604   \n",
      "1             Manufacturing    Male      Asian   59 1997-11-29          99975   \n",
      "2       Speciality Products  Female  Caucasian   50 2006-10-26         163099   \n",
      "3             Manufacturing  Female  Caucasian   26 2019-09-27          84913   \n",
      "4             Manufacturing    Male      Asian   55 1995-11-20          95409   \n",
      "..                      ...     ...        ...  ...        ...            ...   \n",
      "995     Speciality Products    Male  Caucasian   33 2016-09-18          98427   \n",
      "996     Speciality Products  Female      Asian   44 2010-05-31          47387   \n",
      "997     Speciality Products    Male      Asian   31 2019-06-10         176710   \n",
      "998     Speciality Products  Female      Asian   33 2012-01-28          95960   \n",
      "999               Corporate  Female      Asian   63 2020-07-26         216195   \n",
      "\n",
      "     Bonus %        Country       City  Exit Date GetsBonus  \n",
      "0       0.15  United States    Seattle 2021-10-16     bonus  \n",
      "1       0.00          China  Chongqing        NaT  no bonus  \n",
      "2       0.20  United States    Chicago        NaT     bonus  \n",
      "3       0.07  United States    Chicago        NaT     bonus  \n",
      "4       0.00  United States    Phoenix        NaT  no bonus  \n",
      "..       ...            ...        ...        ...       ...  \n",
      "995     0.00  United States   Columbus        NaT  no bonus  \n",
      "996     0.00          China    Chengdu 2018-01-08  no bonus  \n",
      "997     0.15  United States      Miami        NaT     bonus  \n",
      "998     0.00          China    Chengdu        NaT  no bonus  \n",
      "999     0.31  United States      Miami        NaT     bonus  \n",
      "\n",
      "[1000 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# we need a column which will tell us who get bonus and who doesnt\n",
    "#use .loc\n",
    "\n",
    "#if Bonus % value = 0, then in the newly created column 'GetsBonus' print 'no bonus'\n",
    "df.loc[(df[\"Bonus %\"] == 0), \"GetsBonus\"] = \"no bonus\"\n",
    "\n",
    "#if Bonus % value > 0, then in the newly created column 'GetsBonus' print 'bonus'\n",
    "df.loc[(df[\"Bonus %\"] > 0), \"GetsBonus\"] = \"bonus\"\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f431dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary\n",
      "0  EMP01    ayushi      F      NaN\n",
      "1  EMP02     rohit      M  25000.0\n",
      "2  EMP03  pranjali    NaN  27000.0\n",
      "3  EMP01    ayushi      F  20000.0\n",
      "4  EMP05       NaN      M  25000.0\n",
      "5  EMP06     rohit      M      NaN\n",
      "6  EMP02     rohit      M  25000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"company1.csv\")\n",
    "print(data)\n",
    "\n",
    "data[\"name&gender\"] = data[\"Name\"] + data[\"gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7807e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    EEID      Name gender   salary name&gender\n",
      "0  EMP01    ayushi      F      NaN    ayushi F\n",
      "1  EMP02     rohit      M  25000.0     rohit M\n",
      "2  EMP03  pranjali    NaN  27000.0         NaN\n",
      "3  EMP01    ayushi      F  20000.0    ayushi F\n",
      "4  EMP05       NaN      M  25000.0         NaN\n",
      "5  EMP06     rohit      M      NaN     rohit M\n",
      "6  EMP02     rohit      M  25000.0     rohit M\n"
     ]
    }
   ],
   "source": [
    "#we can also concatinate 2 rows and create a new row\n",
    "\n",
    "data[\"name&gender\"] = data[\"Name\"] + \" \" + data[\"gender\"]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75101774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Months\n",
      "0   January\n",
      "1  February\n",
      "2     March\n",
      "3     April\n",
      " \n",
      "     Months Short Form\n",
      "0   January        Jan\n",
      "1  February        Feb\n",
      "2     March        Mar\n",
      "3     April        Apr\n"
     ]
    }
   ],
   "source": [
    "# use of map is to apply a particular function on all the values\n",
    "\n",
    "data = {\"Months\":[\"January\",\"February\",\"March\",\"April\"]}\n",
    "\n",
    "a=pd.DataFrame(data)\n",
    "print(a)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "def extract(value):\n",
    "    return value[0:3]\n",
    "\n",
    "a[\"Short Form\"]=a[\"Months\"].map(extract) #using map() we can apply 'extract' function on all the values of that column\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b5b3b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     EEID        Full Name                 Job Title  Department  \\\n",
      "0  E02387      Emily Davis                Sr. Manger          IT   \n",
      "1  E04105    Theodore Dinh       Technical Architect          IT   \n",
      "2  E02572     Luna Sanders                  Director     Finance   \n",
      "3  E02832  Penelope Jordan  Computer Systems Manager          IT   \n",
      "4  E01639        Austin Vo               Sr. Analyst     Finance   \n",
      "5  E00644     Joshua Gupta    Account Representative       Sales   \n",
      "6  E01550      Ruby Barnes                   Manager          IT   \n",
      "7  E04332      Luke Martin                   Analyst     Finance   \n",
      "8  E04533    Easton Bailey                   Manager  Accounting   \n",
      "9  E03838  Madeline Walker               Sr. Analyst     Finance   \n",
      "\n",
      "            Business Unit  Gender  Ethnicity  Age  Hire Date  Annual Salary  \\\n",
      "0  Research & Development  Female      Black   55 2016-04-08         141604   \n",
      "1           Manufacturing    Male      Asian   59 1997-11-29          99975   \n",
      "2     Speciality Products  Female  Caucasian   50 2006-10-26         163099   \n",
      "3           Manufacturing  Female  Caucasian   26 2019-09-27          84913   \n",
      "4           Manufacturing    Male      Asian   55 1995-11-20          95409   \n",
      "5               Corporate    Male      Asian   57 2017-01-24          50994   \n",
      "6               Corporate  Female  Caucasian   27 2020-07-01         119746   \n",
      "7           Manufacturing    Male      Black   25 2020-05-16          41336   \n",
      "8           Manufacturing    Male  Caucasian   29 2019-01-25         113527   \n",
      "9     Speciality Products  Female  Caucasian   34 2018-06-13          77203   \n",
      "\n",
      "   Bonus %        Country       City  Exit Date  \n",
      "0     0.15  United States    Seattle 2021-10-16  \n",
      "1     0.00          China  Chongqing        NaT  \n",
      "2     0.20  United States    Chicago        NaT  \n",
      "3     0.07  United States    Chicago        NaT  \n",
      "4     0.00  United States    Phoenix        NaT  \n",
      "5     0.00          China  Chongqing        NaT  \n",
      "6     0.10  United States    Phoenix        NaT  \n",
      "7     0.00  United States      Miami 2021-05-20  \n",
      "8     0.06  United States     Austin        NaT  \n",
      "9     0.00  United States    Chicago        NaT  \n"
     ]
    }
   ],
   "source": [
    "#GROUP BY\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_excel(\"ESD.xlsx\")\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a8928b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Gender\n",
      "Department             \n",
      "Accounting           96\n",
      "Engineering         158\n",
      "Finance             120\n",
      "Human Resources     125\n",
      "IT                  241\n",
      "Marketing           120\n",
      "Sales               140\n"
     ]
    }
   ],
   "source": [
    "# ex - using group by function to find count of gender in each department\n",
    "\n",
    "gp = data.groupby(\"Department\").agg({\"Gender\":\"count\"})\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd65977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 EEID\n",
      "Department           \n",
      "Accounting         96\n",
      "Engineering       158\n",
      "Finance           120\n",
      "Human Resources   125\n",
      "IT                241\n",
      "Marketing         120\n",
      "Sales             140\n"
     ]
    }
   ],
   "source": [
    "# ex - we want to find the number of employees in the department\n",
    "# by displaying count of employee ID in each job title\n",
    "\n",
    "gp = data.groupby(\"Department\").agg({\"EEID\":\"count\"})\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "831a243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        EEID\n",
      "Department      Gender      \n",
      "Accounting      Female    53\n",
      "                Male      43\n",
      "Engineering     Female    80\n",
      "                Male      78\n",
      "Finance         Female    69\n",
      "                Male      51\n",
      "Human Resources Female    64\n",
      "                Male      61\n",
      "IT              Female   119\n",
      "                Male     122\n",
      "Marketing       Female    57\n",
      "                Male      63\n",
      "Sales           Female    76\n",
      "                Male      64\n"
     ]
    }
   ],
   "source": [
    "# to display the number of employees in each department but gender wise\n",
    "# you need to group by both department and gender\n",
    "\n",
    "gp = data.groupby([\"Department\",\"Gender\"]).agg({\"EEID\":\"count\"})\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c614fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Annual Salary\n",
      "Country                     \n",
      "Brazil         112324.834532\n",
      "China          113823.532110\n",
      "United States  113204.794712\n"
     ]
    }
   ],
   "source": [
    "# you want to find the mean of annual salary for each country\n",
    "\n",
    "gp1 = data.groupby(\"Country\").agg({\"Annual Salary\":\"mean\"})\n",
    "print(gp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b539cecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Annual Salary  Age\n",
      "Country       Gender                    \n",
      "Brazil        Female         258426   25\n",
      "              Male           249506   26\n",
      "China         Female         249686   25\n",
      "              Male           257194   25\n",
      "United States Female         258498   25\n",
      "              Male           258081   25\n"
     ]
    }
   ],
   "source": [
    "# you want to find the max Annual Salary and min Age for each country\n",
    "# but gender wise\n",
    "\n",
    "gp2 = data.groupby([\"Country\",\"Gender\"]).agg({\"Annual Salary\":\"max\",\"Age\":\"min\"})\n",
    "print(gp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "378a5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID   Names  Age\n",
      "0    E01     Ram   34\n",
      "1    E07   Shyam   56\n",
      "2    E03   Rahul   67\n",
      "3    E10  Vishal   43\n",
      "4    E05    Ravi   21\n",
      " \n",
      "  Emp ID  Salary\n",
      "0    E01    1134\n",
      "1    E02    2256\n",
      "2    E03    4467\n",
      "3    E08    5643\n",
      "4    E05    6721\n"
     ]
    }
   ],
   "source": [
    "#MERGE, CONCATENATE AND JOIN\n",
    "\n",
    "import pandas as pd\n",
    "data1 = {\"Emp ID\":[\"E01\", \"E07\", \"E03\", \"E10\", \"E05\"],\n",
    "        \"Names\":[\"Ram\", \"Shyam\", \"Rahul\",\"Vishal\", \"Ravi\"],\n",
    "        \"Age\":[34, 56, 67, 43, 21]}\n",
    "\n",
    "data2 = {\"Emp ID\":[\"E01\", \"E02\", \"E03\", \"E08\", \"E05\"],\n",
    "        \"Salary\":[1134, 2256, 4467, 5643, 6721]}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "print(df1)\n",
    "print(\" \")\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0697865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID  Names  Age  Salary\n",
      "0    E01    Ram   34    1134\n",
      "1    E03  Rahul   67    4467\n",
      "2    E05   Ravi   21    6721\n"
     ]
    }
   ],
   "source": [
    "# you can merge 2 tables based on some common row using .merge\n",
    "\n",
    "print(pd.merge(df1, df2, on = \"Emp ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e8ab34f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID   Names  Age  Salary\n",
      "0    E01     Ram   34  1134.0\n",
      "1    E07   Shyam   56     NaN\n",
      "2    E03   Rahul   67  4467.0\n",
      "3    E10  Vishal   43     NaN\n",
      "4    E05    Ravi   21  6721.0\n"
     ]
    }
   ],
   "source": [
    "#merging 2 tables based on left you can get the common values plus whatever is specified on the left table\n",
    "# here the common E01,E03 and E05 is printed along with E07 & E10 that are part of df1 only \n",
    "print(pd.merge(left=df1, right=df2, on = \"Emp ID\", how = \"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75af6c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID  Names   Age  Salary\n",
      "0    E01    Ram  34.0    1134\n",
      "1    E02    NaN   NaN    2256\n",
      "2    E03  Rahul  67.0    4467\n",
      "3    E08    NaN   NaN    5643\n",
      "4    E05   Ravi  21.0    6721\n"
     ]
    }
   ],
   "source": [
    "# here the common E01,E02, E03,and E05 is printed along with E08 that is part of df2 only \n",
    "\n",
    "print(pd.merge(left=df1, right=df2, on = \"Emp ID\", how = \"right\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "086d52fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID   Names  Age\n",
      "0    E01     Ram   34\n",
      "1    E02   Shyam   56\n",
      "2    E03   Rahul   67\n",
      "3    E04  Vishal   43\n",
      "4    E05    Ravi   21\n",
      " \n",
      "  Emp ID   Names  Age\n",
      "0    E06    Peru   43\n",
      "1    E07  Chintu   65\n",
      "2    E08   Pavan   76\n",
      "3    E09    Amma   34\n",
      "4    E10    Appa   12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data1 = {\"Emp ID\":[\"E01\", \"E02\", \"E03\", \"E04\", \"E05\"],\n",
    "        \"Names\":[\"Ram\", \"Shyam\", \"Rahul\",\"Vishal\", \"Ravi\"],\n",
    "        \"Age\":[34, 56, 67, 43, 21]}\n",
    "\n",
    "data2 = {\"Emp ID\":[\"E06\", \"E07\", \"E08\", \"E09\", \"E10\"],\n",
    "        \"Names\":[\"Peru\", \"Chintu\", \"Pavan\",\"Amma\", \"Appa\"],\n",
    "        \"Age\":[43, 65, 76, 34, 12]}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "print(df1)\n",
    "print(\" \")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51168058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emp ID   Names  Age\n",
      "0    E01     Ram   34\n",
      "1    E02   Shyam   56\n",
      "2    E03   Rahul   67\n",
      "3    E04  Vishal   43\n",
      "4    E05    Ravi   21\n",
      "0    E06    Peru   43\n",
      "1    E07  Chintu   65\n",
      "2    E08   Pavan   76\n",
      "3    E09    Amma   34\n",
      "4    E10    Appa   12\n"
     ]
    }
   ],
   "source": [
    "# to join the tables use .concat([])\n",
    "\n",
    "print(pd.concat([df1, df2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a30048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPARE DATAFRAMES\n",
    "#PIVOTING MELTING DATA FRAMES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
